name: CI/CD for Airflow DAG

on:
  push:
    paths:
      - 'dags/**' # Trigger the workflow only when changes occur in the 'dags' directory

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Tests
        run: python -m unittest discover -s tests -p 'test_*.py'

  deploy:
    name: Deploy DAG to S3
    runs-on: ubuntu-latest
    needs: test
    if: success()

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3

      - name: Deploy DAG to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-west-1
        run: |
          aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
          aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
          aws configure set default.region $AWS_DEFAULT_REGION
          aws s3 cp dags/faker_etl.py s3://tf-mwaa-airflow-bucket/dags/faker_etl.py
        
